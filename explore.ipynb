{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -q -r requirements.txt --no-cache-dir --upgrade --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Re-read the CSV file, skipping any problematic rows\n",
        "BOOKS = pd.read_csv(\"/Users/nathanielani/library_dataset/books.csv\", encoding='ISO-8859-1', sep=';', quotechar='\"', on_bad_lines='skip')\n",
        "RATINGS = pd.read_csv(\"/Users/nathanielani/library_dataset/ratings.csv\", encoding='ISO-8859-1', sep=';', quotechar='\"', on_bad_lines='skip')\n",
        "USERS = pd.read_csv(\"/Users/nathanielani/library_dataset/users.csv\", encoding='ISO-8859-1', sep=';', quotechar='\"', on_bad_lines='skip')\n",
        "\n",
        "# Print the column names to check if the data is loaded correctly\n",
        "print(\"Books:\", BOOKS.columns)\n",
        "print(\"Ratings:\", RATINGS.columns)\n",
        "print(\"Users:\", USERS.columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print the first few rows of the BOOKS DataFrame to inspect the data\n",
        "print(BOOKS.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean column names by stripping any extra spaces or invisible characters\n",
        "BOOKS.columns = BOOKS.columns.str.strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for any hidden characters in column names\n",
        "for col in BOOKS.columns:\n",
        "    print(f\"'{col}'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get unique values, excluding NaN values\n",
        "isbns = BOOKS[\"ISBN\"].dropna().unique()\n",
        "print(\"Unique ISBNs:\", len(isbns))  # Expected: 271379\n",
        "\n",
        "book_titles = BOOKS[\"Book-Title\"].dropna().unique()\n",
        "print(\"Unique Book Titles:\", len(book_titles))  # Expected: 242135\n",
        "\n",
        "book_authors = BOOKS[\"Book-Author\"].dropna().unique()\n",
        "print(\"Unique Book Authors:\", len(book_authors))  # Expected: 102023\n",
        "\n",
        "year_of_publications = BOOKS[\"Year-Of-Publication\"].dropna().unique()\n",
        "print(\"Unique Year of Publications:\", len(year_of_publications))  # Expected: 202\n",
        "\n",
        "publisher = BOOKS[\"Publisher\"].dropna().unique()\n",
        "print(\"Unique Publishers:\", len(publisher))  # Expected: 16807\n",
        "\n",
        "image_url_m = BOOKS[\"Image-URL-M\"].dropna().unique()\n",
        "print(\"Unique Image URLs (M):\", len(image_url_m))  # Expected: 271379\n",
        "\n",
        "image_url_l = BOOKS[\"Image-URL-L\"].dropna().unique()\n",
        "print(\"Unique Image URLs (L):\", len(image_url_l))  # Expected: 271379\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Define paths\n",
        "INPUT_PATH = \"/Users/nathanielani/library_dataset/\"\n",
        "OUTPUT_PATH = \"/Users/nathanielani/library_dataset/processed/\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "\n",
        "# Load Books.csv with a specified encoding and skip bad lines\n",
        "books_df = pd.read_csv(os.path.join(INPUT_PATH, \"books.csv\"), encoding='ISO-8859-1', sep=';', quotechar='\"', on_bad_lines='skip', low_memory=False)\n",
        "\n",
        "# Display columns to verify structure\n",
        "print(\"Columns in Books.csv:\", books_df.columns)\n",
        "\n",
        "# Rename columns for PostgreSQL compatibility\n",
        "rename_dict = {\n",
        "    \"ISBN\": \"isbn\",\n",
        "    \"Book-Title\": \"book_title\",\n",
        "    \"Book-Author\": \"book_author\",\n",
        "    \"Year-Of-Publication\": \"year_of_publication\",\n",
        "    \"Publisher\": \"publisher\",\n",
        "    \"Image-URL-S\": \"image_url_s\",\n",
        "    \"Image-URL-M\": \"image_url_m\",\n",
        "    \"Image-URL-L\": \"image_url_l\",\n",
        "}\n",
        "\n",
        "books_df.rename(columns=rename_dict, inplace=True)\n",
        "\n",
        "# Remove duplicates (based on ISBN since it's unique for books)\n",
        "books_df.drop_duplicates(subset=[\"isbn\"], inplace=True)\n",
        "\n",
        "# Save cleaned dataset\n",
        "cleaned_file_path = os.path.join(OUTPUT_PATH, \"clean_books.csv\")\n",
        "books_df.to_csv(cleaned_file_path, index=False)\n",
        "\n",
        "print(f\"✅ Dataset has been cleaned and saved to: {cleaned_file_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "user_id = RATINGS[\"User-ID\"].unique()\n",
        "print(\"Unique User IDs:\", len(user_id)) # 105283\n",
        "\n",
        "isbn = RATINGS[\"ISBN\"].unique()\n",
        "print(\"Unique ISBNs:\", len(isbn)) # 340556\n",
        "\n",
        "book_ratings = RATINGS[\"Book-Rating\"].unique()\n",
        "print(\"Unique Book Ratings:\", len(book_ratings)) # 11"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Define paths\n",
        "INPUT_PATH = \"/Users/nathanielani/library_dataset/\"\n",
        "OUTPUT_PATH = \"/Users/nathanielani/library_dataset/processed/\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "\n",
        "# Load Ratings.csv with a specified encoding and skip bad lines\n",
        "ratings_df = pd.read_csv(os.path.join(INPUT_PATH, \"Ratings.csv\"), encoding='ISO-8859-1', sep=';', quotechar='\"', on_bad_lines='skip', low_memory=False)\n",
        "\n",
        "# Display columns to verify structure\n",
        "print(\"Columns in RATINGS.csv:\", ratings_df.columns)\n",
        "\n",
        "# Rename columns for PostgreSQL compatibility\n",
        "rename_dict = {\n",
        "    \"User-ID\": \"user_id\",\n",
        "    \"ISBN\": \"isbn\",\n",
        "    \"Book-Rating\": \"book_rating\",\n",
        "}\n",
        "\n",
        "ratings_df.rename(columns=rename_dict, inplace=True)\n",
        "\n",
        "# Remove duplicates (based on ISBN since it's unique for books)\n",
        "ratings_df.drop_duplicates(subset=[\"isbn\"], inplace=True)\n",
        "\n",
        "# Save cleaned dataset\n",
        "cleaned_file_path = os.path.join(OUTPUT_PATH, \"clean_ratings.csv\")\n",
        "ratings_df.to_csv(cleaned_file_path, index=False)\n",
        "\n",
        "print(f\"✅ Dataset has been cleaned and saved to: {cleaned_file_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "user_id = USERS[\"User-ID\"].unique()\n",
        "print(\"Unique User IDs:\", len(user_id)) # 278858\n",
        "\n",
        "location = USERS[\"Location\"].unique()\n",
        "print(\"Unique Locations:\", len(location)) # 57339\n",
        "\n",
        "age = USERS[\"Age\"].unique()\n",
        "print(\"Unique Ages:\", len(age)) # 166"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Define paths\n",
        "INPUT_PATH = \"/Users/nathanielani/library_dataset/\"\n",
        "OUTPUT_PATH = \"/Users/nathanielani/library_dataset/processed/\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "\n",
        "# Load Users.csv with correct delimiter and quote handling\n",
        "users_df = pd.read_csv(os.path.join(INPUT_PATH, \"Users.csv\"), encoding='ISO-8859-1', sep=';', quotechar='\"', low_memory=False, on_bad_lines='skip')\n",
        "\n",
        "# Display columns to verify structure before renaming\n",
        "print(\"Columns in Users.csv:\", users_df.columns)\n",
        "\n",
        "# Rename columns for PostgreSQL compatibility\n",
        "rename_dict = {\n",
        "    \"User-ID\": \"user_id\",\n",
        "    \"Location\": \"location\",\n",
        "    \"Age\": \"age\",\n",
        "}\n",
        "\n",
        "# Apply renaming\n",
        "users_df.rename(columns=rename_dict, inplace=True)\n",
        "\n",
        "# Display columns after renaming\n",
        "print(\"Columns after renaming:\", users_df.columns)\n",
        "\n",
        "# Remove duplicates (based on user_id)\n",
        "users_df.drop_duplicates(subset=[\"user_id\"], inplace=True)\n",
        "\n",
        "# Save cleaned dataset\n",
        "cleaned_file_path = os.path.join(OUTPUT_PATH, \"clean_users.csv\")\n",
        "users_df.to_csv(cleaned_file_path, index=False)\n",
        "\n",
        "print(f\"✅ Dataset has been cleaned and saved to: {cleaned_file_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Define the directory containing processed files\n",
        "PROCESSED_PATH = \"/Users/nathanielani/library_dataset/processed/\"\n",
        "\n",
        "# List all files in the processed directory\n",
        "processed_files = [f for f in os.listdir(PROCESSED_PATH) if f.endswith(\".csv\")]\n",
        "\n",
        "# Check columns for each CSV file\n",
        "for file in processed_files:\n",
        "    file_path = os.path.join(PROCESSED_PATH, file)\n",
        "    df = pd.read_csv(file_path, nrows=5)  # Read first 5 rows for quick check\n",
        "    print(f\"\\n📂 {file} - Columns:\")\n",
        "    print(df.columns.tolist())  # Print column names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Define paths (SQLAlchemy)\n",
        "INPUT_PATH = \"/Users/nathanielani/library_dataset/processed/\"\n",
        "OUTPUT_PATH = \"/Users/nathanielani/library_dataset/sql-files/\"\n",
        "\n",
        "\n",
        "# Define Table Names\n",
        "\n",
        "csv_files = {\"books\": \"clean_books.csv\", \"ratings\": \"clean_ratings.csv\", \"users\": \"clean_users.csv\"}\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "\n",
        "for table_name, csv_file in csv_files.items():\n",
        "    # Load cleaned CSV file\n",
        "    df = pd.read_csv(os.path.join(INPUT_PATH, csv_file))\n",
        "\n",
        "    # Define SQL file path\n",
        "    sql_file_path = os.path.join(OUTPUT_PATH, f\"{table_name}.sql\")\n",
        "\n",
        "    # Generate SQL CREATE TABLE statement\n",
        "    create_table_sql = f\"CREATE TABLE IF NOT EXISTS {table_name} (\\n\"\n",
        "\n",
        "    for column in df.columns:\n",
        "        # Get column data type\n",
        "        data_type = df[column].dtype\n",
        "\n",
        "        # Define SQL column definition\n",
        "        if \"object\" in str(data_type):\n",
        "            create_table_sql += f\"    {column} VARCHAR,\\n\"\n",
        "        elif \"int\" in str(data_type):\n",
        "            create_table_sql += f\"    {column} INTEGER,\\n\"\n",
        "        elif \"float\" in str(data_type):\n",
        "            create_table_sql += f\"    {column} FLOAT,\\n\"\n",
        "\n",
        "    # Finalize CREATE TABLE statement\n",
        "    create_table_sql = create_table_sql.strip().strip(\",\") + \"\\n);\"\n",
        "\n",
        "    # Save SQL file\n",
        "    with open(sql_file_path, \"w\") as f:\n",
        "        f.write(create_table_sql)\n",
        "\n",
        "    print(f\"✅ {table_name} - SQL file has been created: {sql_file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Define input/output paths\n",
        "INPUT_PATH = \"/Users/nathanielani/library_dataset/processed/\"\n",
        "OUTPUT_PATH = \"/Users/nathanielani/library_dataset/sql-files/\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "\n",
        "# Define CSV files and corresponding table names\n",
        "csv_files = {\n",
        "    \"books\": \"clean_books.csv\",\n",
        "    \"ratings\": \"clean_ratings.csv\",\n",
        "    \"users\": \"clean_users.csv\",\n",
        "}\n",
        "\n",
        "\n",
        "# Function to map Pandas data types to SQL data types\n",
        "def map_dtype(dtype):\n",
        "    if \"int\" in str(dtype):\n",
        "        return \"BIGINT\"  # Use BIGINT for large user_id values\n",
        "    elif \"float\" in str(dtype):\n",
        "        return \"FLOAT\"\n",
        "    elif \"object\" in str(dtype):\n",
        "        return \"TEXT\"  # TEXT is more flexible than VARCHAR\n",
        "    else:\n",
        "        return \"TEXT\"  # Default fallback for unknown types\n",
        "\n",
        "\n",
        "# Iterate over CSV files and generate SQL schema\n",
        "for table_name, csv_file in csv_files.items():\n",
        "    df = pd.read_csv(os.path.join(INPUT_PATH, csv_file))\n",
        "\n",
        "    # Define SQL file path\n",
        "    sql_file_path = os.path.join(OUTPUT_PATH, f\"{table_name}.sql\")\n",
        "\n",
        "    # Generate SQL CREATE TABLE statement\n",
        "    create_table_sql = f\"DROP TABLE IF EXISTS {table_name};\\n\"\n",
        "    create_table_sql += f\"CREATE TABLE {table_name} (\\n\"\n",
        "\n",
        "    # Identify primary keys based on table\n",
        "    primary_keys = {\"books\": \"isbn\", \"users\": \"user_id\", \"ratings\": \"user_id, isbn\"}\n",
        "\n",
        "    # Generate column definitions\n",
        "    column_definitions = []\n",
        "    for column in df.columns:\n",
        "        sql_type = map_dtype(df[column].dtype)\n",
        "        column_definitions.append(f\"    {column} {sql_type}\")\n",
        "\n",
        "    # Add Primary Key constraint\n",
        "    create_table_sql += \",\\n\".join(column_definitions)\n",
        "    if table_name in primary_keys:\n",
        "        create_table_sql += f\",\\n    PRIMARY KEY ({primary_keys[table_name]})\"\n",
        "\n",
        "    # Finalize SQL schema\n",
        "    create_table_sql += \"\\n);\\n\"\n",
        "\n",
        "    # Save SQL file\n",
        "    with open(sql_file_path, \"w\") as f:\n",
        "        f.write(create_table_sql)\n",
        "\n",
        "    print(f\"✅ {table_name}.sql file has been created: {sql_file_path}\")\n",
        "\n",
        "print(\"\\n🚀 All SQL schema files have been generated successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "image_path = \"/Users/timeless/library_dataset/recsys_taxonomy2.png\"\n",
        "img = Image.open(image_path)\n",
        "\n",
        "# Display the image\n",
        "plt.imshow(img)\n",
        "plt.axis(\"off\")  # Hide axes\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import hashlib\n",
        "import os\n",
        "\n",
        "class PasswordHasher:\n",
        "    def __init__(self):\n",
        "        self.salt = os.urandom(32)  # Generate a random 32-byte salt\n",
        "\n",
        "    def hash_password(self, password):\n",
        "        # Use SHA-256 hashing algorithm\n",
        "        key = hashlib.pbkdf2_hmac(\n",
        "            \"sha256\",  # The hash digest algorithm for HMAC\n",
        "            password.encode(\"utf-8\"),  # Convert the password to bytes\n",
        "            self.salt,  # Provide the salt\n",
        "            100000,  # It is recommended to use at least 100,000 iterations of SHA-256\n",
        "        )\n",
        "        return key\n",
        "\n",
        "    def verify_password(self, hashed_password, provided_password):\n",
        "        # Hash the provided password\n",
        "        new_hashed_password = self.hash_password(provided_password)\n",
        "\n",
        "        # Compare the hashed passwords\n",
        "        return new_hashed_password == hashed_password\n",
        "\n",
        "    def get_user_input(self):\n",
        "        # Get user input for password\n",
        "        password = input(\"Enter your password: \")\n",
        "\n",
        "        print(\"Plain password entered:\", password)\n",
        "\n",
        "        # Hash the password\n",
        "        hashed_password = self.hash_password(password)\n",
        "\n",
        "        # Verify the password\n",
        "        if self.verify_password(hashed_password, password):\n",
        "            print(\"Password verified successfully!\")\n",
        "        else:\n",
        "            print(\"Password verification failed!\")\n",
        "\n",
        "        return hashed_password"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hasher = PasswordHasher()\n",
        "\n",
        "# Get user input and hash the password\n",
        "hashed_password = hasher.get_user_input()\n",
        "\n",
        "print(\"Hashed Password:\", hashed_password)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hasher = PasswordHasher()\n",
        "\n",
        "# Define a password\n",
        "password = \"mysecretpassword\"\n",
        "\n",
        "# Hash the password\n",
        "hashed_password = hasher.hash_password(password)\n",
        "print(f\"Hashed Password: {hashed_password}\")\n",
        "\n",
        "verifify_password = hasher.verify_password(hashed_password, password)\n",
        "print(f\"Password Verified: {verifify_password}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Strong Password: {Xa'V@#|Pi%9\n"
          ]
        }
      ],
      "source": [
        "# Generate a strong password froma password library\n",
        "import secrets\n",
        "import string\n",
        "\n",
        "def generate_password(length=12):\n",
        "    alphabet = string.ascii_letters + string.digits + string.punctuation\n",
        "    password = \"\".join(secrets.choice(alphabet) for i in range(length))\n",
        "    return password\n",
        "\n",
        "# Generate a strong password\n",
        "strong_password = generate_password()\n",
        "print(f\"Strong Password: {strong_password}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
