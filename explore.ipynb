{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -r requirements.txt --no-cache-dir --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z7/5d6zsxzx1tb0smj9wjhw8l800000gn/T/ipykernel_26925/1842390125.py:3: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  BOOKS = pd.read_csv(\"/Users/timeless/library_dataset/Books.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boosk: Index(['ISBN', 'Book-Title', 'Book-Author', 'Year-Of-Publication', 'Publisher',\n",
      "       'Image-URL-S', 'Image-URL-M', 'Image-URL-L'],\n",
      "      dtype='object')\n",
      "Ratings: Index(['User-ID', 'ISBN', 'Book-Rating'], dtype='object')\n",
      "Users: Index(['User-ID', 'Location', 'Age'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "BOOKS = pd.read_csv(\"/Users/timeless/library_dataset/Books.csv\")\n",
    "RATINGS = pd.read_csv(\"/Users/timeless/library_dataset/Ratings.csv\")\n",
    "USERS = pd.read_csv(\"/Users/timeless/library_dataset/Users.csv\")\n",
    "\n",
    "print(\"Boosk:\", BOOKS.columns)\n",
    "print(\"Ratings:\", RATINGS.columns)\n",
    "print(\"Users:\", USERS.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique ISBNs: 271360\n",
      "Unique Book Titles: 242135\n",
      "Unique Book Authors: 102023\n",
      "Unique Year of Publications: 202\n",
      "Unique Publishers: 16808\n",
      "Unique Image URLs: 271044\n",
      "Unique Image URLs: 271042\n"
     ]
    }
   ],
   "source": [
    "isbns = BOOKS[\"ISBN\"].unique()\n",
    "print(\"Unique ISBNs:\", len(isbns))  # 271379\n",
    "\n",
    "book_titles = BOOKS[\"Book-Title\"].unique()\n",
    "print(\"Unique Book Titles:\", len(book_titles)) # 242135\n",
    "\n",
    "book_authors = BOOKS[\"Book-Author\"].unique()\n",
    "print(\"Unique Book Authors:\", len(book_authors)) # 102023\n",
    "\n",
    "year_of_publications = BOOKS[\"Year-Of-Publication\"].unique()\n",
    "print(\"Unique Year of Publications:\", len(year_of_publications)) # 202\n",
    "\n",
    "publisher = BOOKS[\"Publisher\"].unique()\n",
    "print(\"Unique Publishers:\", len(publisher)) # 16807\n",
    "\n",
    "image_url_m = BOOKS[\"Image-URL-M\"].unique()\n",
    "print(\"Unique Image URLs:\", len(image_url_m)) # 271379\n",
    "\n",
    "image_url_l = BOOKS[\"Image-URL-L\"].unique()\n",
    "print(\"Unique Image URLs:\", len(image_url_l)) # 271379"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in Books.csv: Index(['ISBN', 'Book-Title', 'Book-Author', 'Year-Of-Publication', 'Publisher',\n",
      "       'Image-URL-S', 'Image-URL-M', 'Image-URL-L'],\n",
      "      dtype='object')\n",
      "✅ Dataset has been cleaned and saved to: /Users/timeless/library_dataset/processed/clean_books.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define paths\n",
    "INPUT_PATH = \"/Users/timeless/library_dataset/\"\n",
    "OUTPUT_PATH = \"/Users/timeless/library_dataset/processed/\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "# Load Books.csv (since separate files don't exist)\n",
    "books_df = pd.read_csv(os.path.join(INPUT_PATH, \"Books.csv\"), low_memory=False)\n",
    "\n",
    "# Display columns to verify structure\n",
    "print(\"Columns in Books.csv:\", books_df.columns)\n",
    "\n",
    "# Rename columns for PostgreSQL compatibility\n",
    "rename_dict = {\n",
    "    \"ISBN\": \"isbn\",\n",
    "    \"Book-Title\": \"book_title\",\n",
    "    \"Book-Author\": \"book_author\",\n",
    "    \"Year-Of-Publication\": \"year_of_publication\",\n",
    "    \"Publisher\": \"publisher\",\n",
    "    \"Image-URL-S\": \"image_url_s\",\n",
    "    \"Image-URL-M\": \"image_url_m\",\n",
    "    \"Image-URL-L\": \"image_url_l\",\n",
    "}\n",
    "\n",
    "books_df.rename(columns=rename_dict, inplace=True)\n",
    "\n",
    "# Remove duplicates (based on ISBN since it's unique for books)\n",
    "books_df.drop_duplicates(subset=[\"isbn\"], inplace=True)\n",
    "\n",
    "# Save cleaned dataset\n",
    "cleaned_file_path = os.path.join(OUTPUT_PATH, \"clean_books.csv\")\n",
    "books_df.to_csv(cleaned_file_path, index=False)\n",
    "\n",
    "print(f\"✅ Dataset has been cleaned and saved to: {cleaned_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique User IDs: 105283\n",
      "Unique ISBNs: 340556\n",
      "Unique Book Ratings: 11\n"
     ]
    }
   ],
   "source": [
    "user_id = RATINGS[\"User-ID\"].unique()\n",
    "print(\"Unique User IDs:\", len(user_id)) # 105283\n",
    "\n",
    "isbn = RATINGS[\"ISBN\"].unique()\n",
    "print(\"Unique ISBNs:\", len(isbn)) # 340556\n",
    "\n",
    "book_ratings = RATINGS[\"Book-Rating\"].unique()\n",
    "print(\"Unique Book Ratings:\", len(book_ratings)) # 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in RATINGS.csv: Index(['User-ID', 'ISBN', 'Book-Rating'], dtype='object')\n",
      "✅ Dataset has been cleaned and saved to: /Users/timeless/library_dataset/processed/clean_ratings.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define paths\n",
    "INPUT_PATH = \"/Users/timeless/library_dataset/\"\n",
    "OUTPUT_PATH = \"/Users/timeless/library_dataset/processed/\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "# Load Books.csv (since separate files don't exist)\n",
    "ratings_df = pd.read_csv(os.path.join(INPUT_PATH, \"Ratings.csv\"), low_memory=False)\n",
    "\n",
    "# Display columns to verify structure\n",
    "print(\"Columns in RATINGS.csv:\", ratings_df.columns)\n",
    "\n",
    "# Rename columns for PostgreSQL compatibility\n",
    "rename_dict = {\n",
    "    \"User-ID\": \"user_id\",\n",
    "    \"ISBN\": \"isbn\",\n",
    "    \"Book-Rating\": \"book_rating\",\n",
    "}\n",
    "\n",
    "ratings_df.rename(columns=rename_dict, inplace=True)\n",
    "\n",
    "# Remove duplicates (based on ISBN since it's unique for books)\n",
    "ratings_df.drop_duplicates(subset=[\"isbn\"], inplace=True)\n",
    "\n",
    "# Save cleaned dataset\n",
    "cleaned_file_path = os.path.join(OUTPUT_PATH, \"clean_ratings.csv\")\n",
    "ratings_df.to_csv(cleaned_file_path, index=False)\n",
    "\n",
    "print(f\"✅ Dataset has been cleaned and saved to: {cleaned_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique User IDs: 278858\n",
      "Unique Locations: 57339\n",
      "Unique Ages: 166\n"
     ]
    }
   ],
   "source": [
    "user_id = USERS[\"User-ID\"].unique()\n",
    "print(\"Unique User IDs:\", len(user_id)) # 278858\n",
    "\n",
    "location = USERS[\"Location\"].unique()\n",
    "print(\"Unique Locations:\", len(location)) # 57339\n",
    "\n",
    "age = USERS[\"Age\"].unique()\n",
    "print(\"Unique Ages:\", len(age)) # 166"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in Users.csv: Index(['User-ID', 'Location', 'Age'], dtype='object')\n",
      "✅ Dataset has been cleaned and saved to: /Users/timeless/library_dataset/processed/clean_users.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define paths\n",
    "INPUT_PATH = \"/Users/timeless/library_dataset/\"\n",
    "OUTPUT_PATH = \"/Users/timeless/library_dataset/processed/\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "# Load Books.csv (since separate files don't exist)\n",
    "users_df = pd.read_csv(os.path.join(INPUT_PATH, \"Users.csv\"), low_memory=False)\n",
    "\n",
    "# Display columns to verify structure\n",
    "print(\"Columns in Users.csv:\", users_df.columns)\n",
    "\n",
    "# Rename columns for PostgreSQL compatibility\n",
    "rename_dict = {\n",
    "    \"User-ID\": \"user_id\",\n",
    "    \"Location\": \"location\",\n",
    "    \"Age\": \"age\",\n",
    "}\n",
    "\n",
    "users_df.rename(columns=rename_dict, inplace=True)\n",
    "\n",
    "# Remove duplicates (based on ISBN since it's unique for books)\n",
    "users_df.drop_duplicates(subset=[\"user_id\"], inplace=True)\n",
    "\n",
    "# Save cleaned dataset\n",
    "cleaned_file_path = os.path.join(OUTPUT_PATH, \"clean_users.csv\")\n",
    "users_df.to_csv(cleaned_file_path, index=False)\n",
    "\n",
    "print(f\"✅ Dataset has been cleaned and saved to: {cleaned_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📂 clean_users.csv - Columns:\n",
      "['user_id', 'location', 'age']\n",
      "\n",
      "📂 clean_books.csv - Columns:\n",
      "['isbn', 'book_title', 'book_author', 'year_of_publication', 'publisher', 'image_url_s', 'image_url_m', 'image_url_l']\n",
      "\n",
      "📂 clean_ratings.csv - Columns:\n",
      "['user_id', 'isbn', 'book_rating']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the directory containing processed files\n",
    "PROCESSED_PATH = \"/Users/timeless/library_dataset/processed/\"\n",
    "\n",
    "# List all files in the processed directory\n",
    "processed_files = [f for f in os.listdir(PROCESSED_PATH) if f.endswith(\".csv\")]\n",
    "\n",
    "# Check columns for each CSV file\n",
    "for file in processed_files:\n",
    "    file_path = os.path.join(PROCESSED_PATH, file)\n",
    "    df = pd.read_csv(file_path, nrows=5)  # Read first 5 rows for quick check\n",
    "    print(f\"\\n📂 {file} - Columns:\")\n",
    "    print(df.columns.tolist())  # Print column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define paths (SQLAlchemy)\n",
    "INPUT_PATH = \"/Users/timeless/library_dataset/processed/\"\n",
    "OUTPUT_PATH = \"/Users/timeless/library_dataset/sql-files/\"\n",
    "\n",
    "\n",
    "# Define Table Names\n",
    "\n",
    "csv_files = {\"books\": \"clean_books.csv\", \"ratings\": \"clean_ratings.csv\", \"users\": \"clean_users.csv\"}\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "for table_name, csv_file in csv_files.items():\n",
    "    # Load cleaned CSV file\n",
    "    df = pd.read_csv(os.path.join(INPUT_PATH, csv_file))\n",
    "\n",
    "    # Define SQL file path\n",
    "    sql_file_path = os.path.join(OUTPUT_PATH, f\"{table_name}.sql\")\n",
    "\n",
    "    # Generate SQL CREATE TABLE statement\n",
    "    create_table_sql = f\"CREATE TABLE IF NOT EXISTS {table_name} (\\n\"\n",
    "\n",
    "    for column in df.columns:\n",
    "        # Get column data type\n",
    "        data_type = df[column].dtype\n",
    "\n",
    "        # Define SQL column definition\n",
    "        if \"object\" in str(data_type):\n",
    "            create_table_sql += f\"    {column} VARCHAR,\\n\"\n",
    "        elif \"int\" in str(data_type):\n",
    "            create_table_sql += f\"    {column} INTEGER,\\n\"\n",
    "        elif \"float\" in str(data_type):\n",
    "            create_table_sql += f\"    {column} FLOAT,\\n\"\n",
    "\n",
    "    # Finalize CREATE TABLE statement\n",
    "    create_table_sql = create_table_sql.strip().strip(\",\") + \"\\n);\"\n",
    "\n",
    "    # Save SQL file\n",
    "    with open(sql_file_path, \"w\") as f:\n",
    "        f.write(create_table_sql)\n",
    "\n",
    "    print(f\"✅ {table_name} - SQL file has been created: {sql_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define input/output paths\n",
    "INPUT_PATH = \"/Users/timeless/library_dataset/processed/\"\n",
    "OUTPUT_PATH = \"/Users/timeless/library_dataset/sql-files/\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "# Define CSV files and corresponding table names\n",
    "csv_files = {\n",
    "    \"books\": \"clean_books.csv\",\n",
    "    \"ratings\": \"clean_ratings.csv\",\n",
    "    \"users\": \"clean_users.csv\",\n",
    "}\n",
    "\n",
    "\n",
    "# Function to map Pandas data types to SQL data types\n",
    "def map_dtype(dtype):\n",
    "    if \"int\" in str(dtype):\n",
    "        return \"BIGINT\"  # Use BIGINT for large user_id values\n",
    "    elif \"float\" in str(dtype):\n",
    "        return \"FLOAT\"\n",
    "    elif \"object\" in str(dtype):\n",
    "        return \"TEXT\"  # TEXT is more flexible than VARCHAR\n",
    "    else:\n",
    "        return \"TEXT\"  # Default fallback for unknown types\n",
    "\n",
    "\n",
    "# Iterate over CSV files and generate SQL schema\n",
    "for table_name, csv_file in csv_files.items():\n",
    "    df = pd.read_csv(os.path.join(INPUT_PATH, csv_file))\n",
    "\n",
    "    # Define SQL file path\n",
    "    sql_file_path = os.path.join(OUTPUT_PATH, f\"{table_name}.sql\")\n",
    "\n",
    "    # Generate SQL CREATE TABLE statement\n",
    "    create_table_sql = f\"DROP TABLE IF EXISTS {table_name};\\n\"\n",
    "    create_table_sql += f\"CREATE TABLE {table_name} (\\n\"\n",
    "\n",
    "    # Identify primary keys based on table\n",
    "    primary_keys = {\"books\": \"isbn\", \"users\": \"user_id\", \"ratings\": \"user_id, isbn\"}\n",
    "\n",
    "    # Generate column definitions\n",
    "    column_definitions = []\n",
    "    for column in df.columns:\n",
    "        sql_type = map_dtype(df[column].dtype)\n",
    "        column_definitions.append(f\"    {column} {sql_type}\")\n",
    "\n",
    "    # Add Primary Key constraint\n",
    "    create_table_sql += \",\\n\".join(column_definitions)\n",
    "    if table_name in primary_keys:\n",
    "        create_table_sql += f\",\\n    PRIMARY KEY ({primary_keys[table_name]})\"\n",
    "\n",
    "    # Finalize SQL schema\n",
    "    create_table_sql += \"\\n);\\n\"\n",
    "\n",
    "    # Save SQL file\n",
    "    with open(sql_file_path, \"w\") as f:\n",
    "        f.write(create_table_sql)\n",
    "\n",
    "    print(f\"✅ {table_name}.sql file has been created: {sql_file_path}\")\n",
    "\n",
    "print(\"\\n🚀 All SQL schema files have been generated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image_path = \"/Users/timeless/library_dataset/recsys_taxonomy2.png\"\n",
    "img = Image.open(image_path)\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")  # Hide axes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
